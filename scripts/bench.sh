#!/bin/bash
# Bullet-proof benchmark script for LLMuxer
# Generates reproducible results with fixed models, dataset, and API endpoints

set -e  # Exit on any error

# Configuration
BENCHMARK_DATE=$(date +%Y%m%d)
BENCHMARK_DIR="benchmarks"
RESULTS_FILE="${BENCHMARK_DIR}/bench_${BENCHMARK_DATE}.json"
MARKDOWN_FILE="docs/benchmarks.md"

echo "üéØ LLMuxer Benchmark Suite"
echo "=========================="
echo "Date: $(date)"
echo "Results file: ${RESULTS_FILE}"
echo "Markdown file: ${MARKDOWN_FILE}"
echo ""

# Check prerequisites
if [ -z "$OPENROUTER_API_KEY" ]; then
    echo "‚ùå Error: OPENROUTER_API_KEY environment variable not set"
    echo "   Get your key from: https://openrouter.ai/keys"
    exit 1
fi

if ! command -v python &> /dev/null; then
    echo "‚ùå Error: Python not found"
    exit 1
fi

# Create benchmark directory
mkdir -p "$BENCHMARK_DIR"

echo "üìä Running benchmark with fixed parameters..."
echo "   ‚Ä¢ Fixed model list (8 models)"
echo "   ‚Ä¢ Fixed dataset: data/jobs_50.jsonl (50 samples)"
echo "   ‚Ä¢ Fixed baseline: openai/gpt-4o"
echo "   ‚Ä¢ Fixed OpenRouter API endpoint"
echo ""

# Run the benchmark
python scripts/bench.py --output "$RESULTS_FILE" --markdown "$MARKDOWN_FILE"

if [ $? -eq 0 ]; then
    echo ""
    echo "‚úÖ Benchmark completed successfully!"
    echo "   Results: ${RESULTS_FILE}"
    echo "   Markdown: ${MARKDOWN_FILE}"
    echo ""
    echo "üìà Summary:"
    python -c "
import json
with open('$RESULTS_FILE', 'r') as f:
    data = json.load(f)
print(f\"   ‚Ä¢ Models tested: {len(data['results'])}\")
print(f\"   ‚Ä¢ Baseline model: {data['config']['baseline_model']}\"")
print(f\"   ‚Ä¢ Dataset size: {data['config']['dataset_size']} samples\"")
print(f\"   ‚Ä¢ Best model: {data['summary']['best_model']['model']}\"")
print(f\"   ‚Ä¢ Cost savings: {data['summary']['best_model']['savings_percent']:.1f}%\"")
"
else
    echo "‚ùå Benchmark failed"
    exit 1
fi